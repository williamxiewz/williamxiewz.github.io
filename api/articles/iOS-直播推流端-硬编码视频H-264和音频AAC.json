{"title":"iOS 直播推流端: 硬编码视频H.264和音频AAC","uid":"65b1423852a0fa523adf6becd9cc334f","slug":"iOS-直播推流端-硬编码视频H-264和音频AAC","date":"2016-09-08T09:53:14.000Z","updated":"2016-09-10T04:03:36.000Z","comments":true,"path":"api/articles/iOS-直播推流端-硬编码视频H-264和音频AAC.json","keywords":null,"cover":null,"content":"<p><a href=\"https://zh.wikipedia.org/zh-cn/%E7%B6%B2%E8%B7%AF%E6%8A%BD%E8%B1%A1%E5%B1%A4#NAL_units\">https://zh.wikipedia.org/zh-cn/網路抽象層#NAL_units</a></p>\n<p>公司项目原因，接触了一下视频流H264的编解码知识，之前项目使用的是FFMpeg多媒体库，利用CPU做视频的编码和解码，俗称为软编软解。该方法比较通用，但是占用CPU资源，编解码效率不高。一般系统都会提供GPU或者专用处理器来对视频流进行编解码，也就是硬件编码和解码，简称为硬编解码。苹果在iOS 8.0系统之前，没有开放系统的硬件编码解码功能，不过Mac OS系统一直有，被称为Video ToolBox的框架来处理硬件的编码和解码，终于在iOS 8.0后，苹果将该框架引入iOS系统。</p>\n<p>由此，开发者便可以在iOS里面，调用Video Toolbox框架提供的接口，来对视频进行硬件编解码的工作，为VOIP视频通话，视频流播放等应用的视频编解码提供了便利。</p>\n<p>（PS：按照苹果WWDC2014 513《direct access to media encoding and decoding》的描述，苹果之前提供的AVFoundation框架也使用硬件对视频进行硬编码和解码，但是编码后直接写入文件，解码后直接显示。Video Toolbox框架可以得到编码后的帧结构，也可以得到解码后的原始图像，因此具有更大的灵活性做一些视频图像处理。）</p>\n<p>一，VideoToolbox基本数据结构。</p>\n<p>Video Toolbox视频编解码前后需要应用的数据结构进行说明。</p>\n<p>（1）CVPixelBuffer：编码前(编码流程)和解码后(解码流程)的图像数据结构,其实就是像素数据。</p>\n<p>（2）CMTime、CMClock和CMTimebase：时间戳相关。时间以64-bit/32-bit的形式出现。</p>\n<p>（3）CMBlockBuffer：编码后，结果图像的数据结构。</p>\n<p>（4）CMVideoFormatDescription：图像存储方式，编解码器等格式描述。</p>\n<p>（5）CMSampleBuffer：存放编解码前后的视频图像的容器数据结构。</p>\n<p>图1.1视频H264编解码前后数据结构示意图</p>\n<p>如图1.1所示，编解码前后的视频图像均封装在CMSampleBuffer中，如果是编码后的图像，以CMBlockBuffe方式存储；解码后的图像，以CVPixelBuffer存储。CMSampleBuffer里面还有另外的时间信息CMTime和视频描述信息CMVideoFormatDesc。</p>\n<p>二，硬解码使用方法。</p>\n<p>通过如图2.1所示的一个典型应用，来说明如何使用硬件解码接口。该应用场景是从网络处传来H264编码后的视频码流，最后显示在手机屏幕上。</p>\n<p>图2.1 H264典型应用场景</p>\n<p>1，将H264码流转换成解码前的CMSampleBuffer。</p>\n<p>由图1.1所示，解码前的CMSampleBuffer = CMTime + FormatDesc + CMBlockBuffer。需要从H264的码流里面提取出以上的三个信息。最后组合成CMSampleBuffer，提供给硬解码接口来进行解码工作。</p>\n<p>H264的码流由NALU单元组成，NALU单元包含视频图像数据和H264的参数信息。其中视频图像数据就是CMBlockBuffer，而H264的参数信息则可以组合成FormatDesc。具体来说参数信息包含SPS（Sequence Parameter Set）和PPS（Picture Parameter Set）。图2.2显示一个H264码流的结构。</p>\n<p>图2.2 H264码流结构</p>\n<p>（1）提取sps和pps生成format description。</p>\n<p>a，每个NALU的开始码是0x00 00 01，按照开始码定位NALU。</p>\n<p>b，通过类型信息找到sps和pps并提取，开始码后第一个byte的后5位，7代表sps，8代表pps。</p>\n<p>c，CMVideoFormatDescriptionCreateFromH264ParameterSets函数来构建CMVideoFormatDescriptionRef。具体代码可以见demo。</p>\n<p>（2）提取视频图像数据生成CMBlockBuffer。</p>\n<p>a，通过开始码，定位到NALU。</p>\n<p>b，确定类型为数据后，将开始码替换成NALU的长度信息（4 Bytes）。</p>\n<p>c，CMBlockBufferCreateWithMemoryBlock接口构造CMBlockBufferRef。具体代码可以见demo。</p>\n<p>（3）根据需要，生成CMTime信息。（实际测试时，加入time信息后，有不稳定的图像，不加入time信息反而没有，需要进一步研究，这里建议不加入time信息）</p>\n<p>根据上述得到CMVideoFormatDescriptionRef、CMBlockBufferRef和可选的时间信息，使用CMSampleBufferCreate接口得到CMSampleBuffer数据这个待解码的原始的数据。见图2.3的H264数据转换示意图。</p>\n<p>图2.3 H264码流转换CMSampleBuffer示意图</p>\n<p>2，硬件解码图像显示。</p>\n<p>硬件解码显示的方式有两种：</p>\n<p>（1）通过系统提供的AVSampleBufferDisplayLayer来解码并显示。</p>\n<p>AVSampleBufferDisplayLayer是苹果提供的一个专门显示编码后的H264数据的显示层，它是CALayer的子类，因此使用方式和其它CALayer类似。该层内置了硬件解码功能，将原始的CMSampleBuffer解码后的图像直接显示在屏幕上面，非常的简单方便。图2.4显示了这一解码过程。</p>\n<p>图2.4 AVSampleBufferDisplayLayer硬解压后显示图像</p>\n<p>显示的接口为[_avslayer enqueueSampleBuffer:sampleBuffer];</p>\n<p>（2）通过VTDecompression接口来，将CMSampleBuffer解码成图像，将图像通过UIImageView或者OpenGL上显示。</p>\n<p>a，初始化VTDecompressionSession，设置解码器的相关信息。初始化信息需要CMSampleBuffer里面的FormatDescription，以及设置解码后图像的存储方式。demo里面设置的CGBitmap模式，使用RGB方式存放。编码后的图像经过解码后，会调用一个回调函数，将解码后的图像交个这个回调函数来进一步处理。我们就在这个回调里面，将解码后的图像发给control来显示，初始化的时候要将回调指针作为参数传给create接口函数。最后使用create接口对session来进行初始化。</p>\n<p>b，a中所述的回调函数可以完成CGBitmap图像转换成UIImage图像的处理，将图像通过队列发送到Control来进行显示处理。</p>\n<p>c，调用VTDecompresSessionDecodeFrame接口进行解码操作。解码后的图像会交由a，b步骤设置的回调函数，来进一步的处理。</p>\n<p>图2.5显示来硬解码的过程步骤。</p>\n<p>图2.5 VTDecompression硬解码过程示意图</p>\n<p>三，硬编码使用方法。</p>\n<p>硬编码的使用也通过一个典型的应用场景来描述。首先，通过摄像头来采集图像，然后将采集到的图像，通过硬编码的方式进行编码，最后编码后的数据将其组合成H264的码流通过网络传播。</p>\n<p>1，摄像头采集数据。</p>\n<p>摄像头采集，iOS系统提供了AVCaptureSession来采集摄像头的图像数据。设定好session的采集解析度。再设定好input和output即可。output设定的时候，需要设置delegate和输出队列。在delegate方法，处理采集好的图像。</p>\n<p>注意，需要说明的是，图像输出的格式，是未编码的CMSampleBuffer形式。</p>\n<p>2，使用VTCompressionSession进行硬编码。</p>\n<p>（1）初始化VTCompressionSession。</p>\n<p>VTCompressionSession初始化的时候，一般需要给出width宽，height长，编码器类型kCMVideoCodecType_H264等。然后通过调用VTSessionSetProperty接口设置帧率等属性，demo里面提供了一些设置参考，测试的时候发现几乎没有什么影响，可能需要进一步调试。最后需要设定一个回调函数，这个回调是视频图像编码成功后调用。全部准备好后，使用VTCompressionSessionCreate创建session。</p>\n<p>（2）提取摄像头采集的原始图像数据给VTCompressionSession来硬编码。</p>\n<p>摄像头采集后的图像是未编码的CMSampleBuffer形式，利用给定的接口函数CMSampleBufferGetImageBuffer从中提取出CVPixelBufferRef，使用硬编码接口VTCompressionSessionEncodeFrame来对该帧进行硬编码，编码成功后，会自动调用session初始化时设置的回调函数。</p>\n<p>（3）利用回调函数，将因编码成功的CMSampleBuffer转换成H264码流，通过网络传播。</p>\n<p>基本上是硬解码的一个逆过程。解析出参数集SPS和PPS，加上开始码后组装成NALU。提取出视频数据，将长度码转换成开始码，组长成NALU。将NALU发送出去。</p>\n<p>图2.6显示了整个硬编码的处理逻辑。</p>\n<p>图2.6硬编码处理流程示意图</p>\n<p>四，硬编解码的一些编码说明。</p>\n<p>由于Video Toolbox是基础的core Foundation库函数，C语言写成，和使用core Foundation所有的其它功能一样需要适应，记得Github有个同志，将其改成了OC语言能方便调用的模式，但是地址忘了，以后有缘找到，就会提供下链接。</p>\n<p>文／Ethan_Struggle（简书作者）<br>原文链接：<a href=\"http://www.jianshu.com/p/a6530fa46a88\">http://www.jianshu.com/p/a6530fa46a88</a><br>著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。</p>\n","text":"https://zh.wikipedia.org/zh-cn/網路抽象層#NAL_units 公司项目原因，接触了一下视频流H264的编解码知识，之前项目使用的是FFMpeg多媒体库，利用CPU做视频的编码和解码，俗称为软编软解。该方法比较通用，但是占用CPU资源，编解码效率不高...","link":"","photos":[],"count_time":{"symbolsCount":"4.1k","symbolsTime":"4 mins."},"categories":[],"tags":[],"toc":"","author":{"name":"William Xie","slug":"blog-author","avatar":"/img/author.png","link":"/","description":"","socials":{"github":"https://github.com/williamxiewz","twitter":"https://twitter.com/williamxie_wz","stackoverflow":"http://stackoverflow.com/users/4078104/goingxiebin-jobs","wechat":"","qq":"","weibo":"https://weibo.com/u/2281381063","zhihu":"https://www.zhihu.com/people/williamxiewz","csdn":"https://blog.csdn.net/u014222645","juejin":"https://juejin.cn/user/3280598430133277","customs":{}}},"mapped":true,"prev_post":{"title":"iOS 直播推流端: 软编码视频H.264和音频AAC","uid":"1acfe8485ebcb89a9ff92dc76b528910","slug":"iOS-直播推流端-软编码视频H-264和音频AAC","date":"2016-09-08T09:53:29.000Z","updated":"2016-09-08T09:59:22.000Z","comments":true,"path":"api/articles/iOS-直播推流端-软编码视频H-264和音频AAC.json","keywords":null,"cover":null,"text":"","link":"","photos":[],"count_time":{"symbolsCount":0,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"William Xie","slug":"blog-author","avatar":"/img/author.png","link":"/","description":"","socials":{"github":"https://github.com/williamxiewz","twitter":"https://twitter.com/williamxie_wz","stackoverflow":"http://stackoverflow.com/users/4078104/goingxiebin-jobs","wechat":"","qq":"","weibo":"https://weibo.com/u/2281381063","zhihu":"https://www.zhihu.com/people/williamxiewz","csdn":"https://blog.csdn.net/u014222645","juejin":"https://juejin.cn/user/3280598430133277","customs":{}}}},"next_post":{"title":"iOS 直播推流端:采集音视频数据","uid":"f7d4fe327521718ef855a7b5e57fec96","slug":"iOS-直播推流端-采集音视频数据","date":"2016-09-08T09:40:42.000Z","updated":"2016-09-08T15:29:05.000Z","comments":true,"path":"api/articles/iOS-直播推流端-采集音视频数据.json","keywords":null,"cover":null,"text":"OS上使用AVFoundation.framework框架来调用系统相机并获取视频数据。视频数据可以根据设定的参数，可采集到RGB或YUV数据，一般使用的是GBRA32，420v，420f，下面演示相机的调用和视频数据的获取。a)引入框架的头文件#import &lt;AVFou...","link":"","photos":[],"count_time":{"symbolsCount":"2.3k","symbolsTime":"2 mins."},"categories":[{"name":"音视频开发","slug":"音视频开发","count":7,"path":"api/categories/音视频开发.json"}],"tags":[{"name":"AVCaptureSession","slug":"AVCaptureSession","count":1,"path":"api/tags/AVCaptureSession.json"},{"name":"AVFoundation","slug":"AVFoundation","count":1,"path":"api/tags/AVFoundation.json"},{"name":"直播推流","slug":"直播推流","count":1,"path":"api/tags/直播推流.json"}],"author":{"name":"William Xie","slug":"blog-author","avatar":"/img/author.png","link":"/","description":"","socials":{"github":"https://github.com/williamxiewz","twitter":"https://twitter.com/williamxie_wz","stackoverflow":"http://stackoverflow.com/users/4078104/goingxiebin-jobs","wechat":"","qq":"","weibo":"https://weibo.com/u/2281381063","zhihu":"https://www.zhihu.com/people/williamxiewz","csdn":"https://blog.csdn.net/u014222645","juejin":"https://juejin.cn/user/3280598430133277","customs":{}}}}}